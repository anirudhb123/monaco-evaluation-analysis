## **MoNaCo Leaderboard**

Below you will find examples of questions and their QDMRs found in Break.   
Note that *high-level* QDMRs are less coarse in their decomposition, as they are geared towards reading comprehension tasks. For more details on *high-level* QDMRs please refer to [our paper](https://allenai.github.io/Break/#paper).

Rank | Model | Date | Link | F1 | Precision | Recall 
------------ | ------------- | ------------- | ------------- | ------------- | ------------- | -------------
1 | o3 | 2025-05 | [🌐](https://openai.com/index/introducing-o3-and-o4-mini/) | **<u>61.18</u>** | **<u>68.10</u>**  | **<u>59.54</u>** 
2 | GPT-5 (medium reasoning) | 2025-08-07 | [🌐](https://openai.com/index/introducing-gpt-5/) | 60.11 | 66.38 | 58.98
3 | Gemini 2.5-Pro | 2025-07 | [🌐](https://deepmind.google/models/gemini/pro/) | 59.11 | 65.02 | 58.14
4 | GPT-4o (few-shot) | 2025-03 | [🌐](https://openai.com/index/hello-gpt-4o/) | 55.05 | 63.33  | 52.88
x | o4-mini | 2025-04 | [🌐](https://openai.com/index/introducing-o3-and-o4-mini/) |  54.92 | 62.50 | 53.01
x | o3-mini | 2025-04 | [🌐](https://openai.com/index/openai-o3-mini/) | 48.75 | 59.29 | 46.19
x | Claude 4-Opus | 2025-07 | [🌐](https://www.anthropic.com/news/claude-4) | 55.03 | 62.28 | 53.47
x | Gemini 2.5-Flash | 2025-07 | [🌐](https://deepmind.google/models/gemini/flash/) | 52.01 | 58.10 | 50.60
x | Deepseek-R1 | 2025-04 | [🌐](https://huggingface.co/deepseek-ai/DeepSeek-R1) | 53.82 | 62.52 | 51.50
x | GPT-4 Turbo (few-shot) | 2024-05 | [🌐](https://platform.openai.com/docs/models/gpt-4-turbo) | 48.58 | 56.26 | 46.81
x | Deepseek-V3 (few-shot) | 2025-05 | [🌐](https://huggingface.co/deepseek-ai/DeepSeek-V3) | 55.04 | 62.31 | 53.37
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14
x | X | x | [🌐](x) | 55.04 | 65.02 | 58.14


